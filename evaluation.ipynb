{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "# functions\n",
    "from functions import *\n",
    "import time\n",
    "# ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"spark\").setMaster(\"local[*,20]\").set(\"spark.driver.memory\", \"10g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "json_folder = \".../datasets/CM-Buildings/*.json\"\n",
    "label_csv = \".../datasets/CM-Buildings/graphs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "    \n",
    "    encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "    # (file_name, prov_types of nodes)\n",
    "    if forward:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "        # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    else:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # (file_name, prov_types occurence in the graph)\n",
    "    types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "    # All prov_types in this collection of graphs\n",
    "    all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "    # Number of distinct prov_types\n",
    "    types_count = len(all_types)\n",
    "    print(types_count)\n",
    "    # index_map for prov_types, prov_type -> index\n",
    "    index_map = {all_types[i]: i for i in range(types_count)}\n",
    "    # index -> prov_type\n",
    "    # Contruct feature vectors for each graph\n",
    "    sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "    feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "    df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scaler_model = scaler.fit(df_features)\n",
    "    df_features = scaler_model.transform(df_features)\n",
    "    # Change the labels\n",
    "    df_labels = spark.read.csv(label_csv, header=True)\n",
    "    df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "    l = list(label_map.values())\n",
    "    df_labels = df_labels.where(df_labels.label.isin(l))\n",
    "    df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "    # Join the features and labels\n",
    "    df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "    # Oversample the training data\n",
    "    labels = [float(x) for x in label_map.values()]\n",
    "    count = {}\n",
    "    for x in labels:\n",
    "        count[x] = df.filter(df['label'] == x).count()\n",
    "    maxValue = max(count.values())\n",
    "    print(maxValue)\n",
    "    ratio = {}\n",
    "    for x in labels:\n",
    "        ratio[x] = maxValue/count[x]\n",
    "    dataframes = []\n",
    "    for x in labels:\n",
    "        if(count[x] == maxValue):\n",
    "            dataframes.append(df.filter(df['label'] == x))\n",
    "        else:\n",
    "            dataframes.append(df.filter(df['label'] == x).sample(withReplacement=True, fraction=ratio[x]))\n",
    "    train = dataframes[0]\n",
    "    for dataframe in dataframes[1:]:\n",
    "        train = train.union(dataframe)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    # LinearSVC classifier\n",
    "    start = time.time()\n",
    "    svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "    pipeline = Pipeline(stages=[svc])\n",
    "    paramGrid = ParamGridBuilder().addGrid(svc.regParam, [0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 200]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [4,5,6]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    end = time.time()\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "    paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    end = time.time()\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "    print(end-start)\n",
    "    start = time.time()\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [5,10]).addGrid(lr.regParam, [0.2,0.3,0.4]).addGrid(lr.elasticNetParam, [0.6,0.7,0.8]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "    print(result)\n",
    "    print(specific_types_edge, specific_types_node,level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 0\n",
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
