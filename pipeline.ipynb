{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "# functions\n",
    "from functions import *\n",
    "# ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \".../datasets/CM-Buildings/*.json\"\n",
    "label_csv = \".../datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"spark\").setMaster(\"local[*]\").set(\"spark.driver.memory\", \"15g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "# spark.sparkContext.addPyFile(\"functions.py\") this may be used in cloud environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe from rdd\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = cvModel.bestModel.stages[0].coefficients\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features(coef.toArray(), reverse_index_map, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [5,10]).addGrid(lr.regParam, [0.2,0.3,0.4]).addGrid(lr.elasticNetParam, [0.6,0.7,0.8]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
